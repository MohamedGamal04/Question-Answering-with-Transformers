# Question Answering with Transformers

This repository provides a comprehensive workflow to train, evaluate, and deploy transformer-based models for extractive Question Answering (QA). The main notebook demonstrates how to fine-tune models like BERT or DistilBERT using Hugging Face's Transformers library on datasets such as SQuAD. It covers dataset preprocessing, model training and evaluation, and interactive inference via command line and Streamlit.

## Features

- End-to-end question answering pipeline using transformer architectures
- Fine-tuning with Hugging Face Trainer API
- Support for popular QA datasets (e.g., SQuAD)
- Evaluation using Exact Match (EM) and F1 metrics
- Interactive inference with simple CLI interface
- Ready-to-adapt for other transformer models and datasets

## Quick Start

1. Clone this repository.
2. Install dependencies:
```powershell
pip install -r requirements.txt
```
3. (Optional) Try the CLI for interactive question answering.

