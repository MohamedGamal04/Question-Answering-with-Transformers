{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSi-4TmrpXAH"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q evaluate streamlit colab-xterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Colab cache for faster access to the 'squad-v11' dataset.\n",
      "Path to dataset files: /kaggle/input/squad-v11\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"akashdesarda/squad-v11\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"data\",\n  \"rows\": 87599,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 442,\n        \"samples\": [\n          \"dominican_order\",\n          \"brigham_young_university\",\n          \"matter\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 18891,\n        \"samples\": [\n          \"more commonly, in cases where there are three or more parties, no one party is likely to gain power alone, and parties work with each other to form coalition governments. this has been an emerging trend in the politics of the republic of ireland since the 1980s and is almost always the case in germany on national and state level, and in most constituencies at the communal level. furthermore, since the forming of the republic of iceland there has never been a government not led by a coalition (usually of the independence party and one other (often the social democratic alliance). a similar situation exists in the republic of ireland; since 1989, no one party has held power on its own. since then, numerous coalition governments have been formed. these coalitions have been exclusively led by one of either fianna f\\u00e1il or fine gael. political change is often easier with a coalition government than in one-party or two-party dominant systems.[dubious \\u2013 discuss] if factions in a two-party system are in fundamental disagreement on policy goals, or even principles, they can be slow to make policy changes, which appears to be the case now in the u.s. with power split between democrats and republicans. still coalition governments struggle, sometimes for years, to change policy and often fail altogether, post world war ii france and italy being prime examples. when one party in a two-party system controls all elective branches, however, policy changes can be both swift and significant. democrats woodrow wilson, franklin roosevelt and lyndon johnson were beneficiaries of such fortuitous circumstances, as were republicans as far removed in time as abraham lincoln and ronald reagan. barack obama briefly had such an advantage between 2009 and 2011.\",\n          \"there has been some concern over the potential adverse environmental and ecosystem effects caused by the influx of visitors. some environmentalists and scientists have made a call for stricter regulations for ships and a tourism quota. the primary response by antarctic treaty parties has been to develop, through their committee for environmental protection and in partnership with iaato, \\\"site use guidelines\\\" setting landing limits and closed or restricted zones on the more frequently visited sites. antarctic sightseeing flights (which did not land) operated out of australia and new zealand until the fatal crash of air new zealand flight 901 in 1979 on mount erebus, which killed all 257 aboard. qantas resumed commercial overflights to antarctica from australia in the mid-1990s.\",\n          \"after world war ii, the guam organic act of 1950 established guam as an unincorporated organized territory of the united states, provided for the structure of the island's civilian government, and granted the people u.s. citizenship. the governor of guam was federally appointed until 1968, when the guam elective governor act provided for the office's popular election.:242 since guam is not a u.s. state, u.s. citizens residing on guam are not allowed to vote for president and their congressional representative is a non-voting member.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 87328,\n        \"samples\": [\n          \"how many types of minerals are found in nanjing?\",\n          \"communications in somalia are mostly concentrated in what sector?\",\n          \"what typically follows the signatures in a treaty?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 64261,\n        \"samples\": [\n          \"the best new building of the year\",\n          \"polish state\",\n          \"1839/40\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_start\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 266,\n        \"min\": 0,\n        \"max\": 3126,\n        \"num_unique_values\": 1604,\n        \"samples\": [\n          527,\n          1,\n          111\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_end\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 267,\n        \"min\": 1,\n        \"max\": 3136,\n        \"num_unique_values\": 1614,\n        \"samples\": [\n          361,\n          1505,\n          863\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "data"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-59831238-0a71-4f83-86fc-c19408fe0e0d\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>university_of_notre_dame</td>\n",
       "      <td>architecturally, the school has a catholic cha...</td>\n",
       "      <td>to whom did the virgin mary allegedly appear i...</td>\n",
       "      <td>saint bernadette soubirous</td>\n",
       "      <td>515</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>university_of_notre_dame</td>\n",
       "      <td>architecturally, the school has a catholic cha...</td>\n",
       "      <td>what is in front of the notre dame main building?</td>\n",
       "      <td>a copper statue of christ</td>\n",
       "      <td>188</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>university_of_notre_dame</td>\n",
       "      <td>architecturally, the school has a catholic cha...</td>\n",
       "      <td>the basilica of the sacred heart at notre dame...</td>\n",
       "      <td>the main building</td>\n",
       "      <td>279</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>university_of_notre_dame</td>\n",
       "      <td>architecturally, the school has a catholic cha...</td>\n",
       "      <td>what is the grotto at notre dame?</td>\n",
       "      <td>a marian place of prayer and reflection</td>\n",
       "      <td>381</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>university_of_notre_dame</td>\n",
       "      <td>architecturally, the school has a catholic cha...</td>\n",
       "      <td>what sits on top of the main building at notre...</td>\n",
       "      <td>a golden statue of the virgin mary</td>\n",
       "      <td>92</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-59831238-0a71-4f83-86fc-c19408fe0e0d')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-59831238-0a71-4f83-86fc-c19408fe0e0d button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-59831238-0a71-4f83-86fc-c19408fe0e0d');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-031e789c-9d55-465c-8fb6-a17d33a405cc\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-031e789c-9d55-465c-8fb6-a17d33a405cc')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-031e789c-9d55-465c-8fb6-a17d33a405cc button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                      title  \\\n",
       "0  university_of_notre_dame   \n",
       "1  university_of_notre_dame   \n",
       "2  university_of_notre_dame   \n",
       "3  university_of_notre_dame   \n",
       "4  university_of_notre_dame   \n",
       "\n",
       "                                             context  \\\n",
       "0  architecturally, the school has a catholic cha...   \n",
       "1  architecturally, the school has a catholic cha...   \n",
       "2  architecturally, the school has a catholic cha...   \n",
       "3  architecturally, the school has a catholic cha...   \n",
       "4  architecturally, the school has a catholic cha...   \n",
       "\n",
       "                                            question  \\\n",
       "0  to whom did the virgin mary allegedly appear i...   \n",
       "1  what is in front of the notre dame main building?   \n",
       "2  the basilica of the sacred heart at notre dame...   \n",
       "3                  what is the grotto at notre dame?   \n",
       "4  what sits on top of the main building at notre...   \n",
       "\n",
       "                                    answer  answer_start  answer_end  \n",
       "0               saint bernadette soubirous           515         541  \n",
       "1                a copper statue of christ           188         213  \n",
       "2                        the main building           279         296  \n",
       "3  a marian place of prayer and reflection           381         420  \n",
       "4       a golden statue of the virgin mary            92         126  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(path+\"/SQuAD-v1.1.csv\")\n",
    "for col in ['title', 'context', 'question', 'answer']:\n",
    "    data[col] = data[col].str.lower()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGFSEaHFuz5v"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8msEar3u1LZ"
   },
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSk9wVLAf6Bq"
   },
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "BERT_model = AutoModelForQuestionAnswering.from_pretrained(\"google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [45:07<00:00,  2.71s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i in tqdm(range(1000)):\n",
    "    question = data[\"question\"][i]\n",
    "    context = data[\"context\"][i]\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = BERT_model(**inputs)\n",
    "        start_logits = outputs.start_logits\n",
    "        end_logits = outputs.end_logits\n",
    "\n",
    "    start_idx = torch.argmax(start_logits)\n",
    "    end_idx = torch.argmax(end_logits) + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
    "    )\n",
    "    predictions.append({answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': 79.1, 'f1': 87.30905782126362}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"squad\")\n",
    "references = [\n",
    "    {\"id\": str(i), \"answers\": {\"text\": [data[\"answer\"][i]], \"answer_start\": [data[\"answer_start\"][i]]}}\n",
    "    for i in range(len(predictions))\n",
    "]\n",
    "formatted_predictions = [{\"id\": str(i), \"prediction_text\": list(predictions[i])[0]} for i in range(len(predictions))]\n",
    "results = metric.compute(predictions=formatted_predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyufWRRff_Ga"
   },
   "source": [
    "## RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "RoBERTa_model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [12:58<00:00,  1.28it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i in tqdm(range(1000)):\n",
    "    question = data[\"question\"][i]\n",
    "    context = data[\"context\"][i]\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = RoBERTa_model(**inputs)\n",
    "        start_logits = outputs.start_logits\n",
    "        end_logits = outputs.end_logits\n",
    "\n",
    "    start_idx = torch.argmax(start_logits)\n",
    "    end_idx = torch.argmax(end_logits) + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
    "    )\n",
    "    predictions.append({answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': 77.8, 'f1': 86.65935352316549}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"squad\")\n",
    "references = [\n",
    "    {\"id\": str(i), \"answers\": {\"text\": [data[\"answer\"][i]], \"answer_start\": [data[\"answer_start\"][i]]}}\n",
    "    for i in range(len(predictions))\n",
    "]\n",
    "formatted_predictions = [{\"id\": str(i), \"prediction_text\": list(predictions[i])[0]} for i in range(len(predictions))]\n",
    "results = metric.compute(predictions=formatted_predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvaBE9V4gDyc"
   },
   "source": [
    "## DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-cased-distilled-squad\")\n",
    "DistilBERT_model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-cased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [06:36<00:00,  2.52it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i in tqdm(range(1000)):\n",
    "    question = data[\"question\"][i]\n",
    "    context = data[\"context\"][i]\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = DistilBERT_model(**inputs)\n",
    "        start_logits = outputs.start_logits\n",
    "        end_logits = outputs.end_logits\n",
    "\n",
    "    start_idx = torch.argmax(start_logits)\n",
    "    end_idx = torch.argmax(end_logits) + 1\n",
    "\n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_idx:end_idx])\n",
    "    )\n",
    "    predictions.append({answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': 69.9, 'f1': 81.23808877582852}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"squad\")\n",
    "references = [\n",
    "    {\"id\": str(i), \"answers\": {\"text\": [data[\"answer\"][i]], \"answer_start\": [data[\"answer_start\"][i]]}}\n",
    "    for i in range(len(predictions))\n",
    "]\n",
    "formatted_predictions = [{\"id\": str(i), \"prediction_text\": list(predictions[i])[0]} for i in range(len(predictions))]\n",
    "results = metric.compute(predictions=formatted_predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THI2oW-xp3fc"
   },
   "source": [
    "# Command-line interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(question, context):\n",
    "  question = question\n",
    "  context = context\n",
    "  inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "  with torch.no_grad():\n",
    "        outputs = BERT_model(**inputs)\n",
    "        start_logits = outputs.start_logits\n",
    "        end_logits = outputs.end_logits\n",
    "  start_idx = torch.argmax(start_logits)\n",
    "  end_idx = torch.argmax(end_logits) + 1\n",
    "  answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][start_idx:end_idx]))\n",
    "  result = [answer, start_idx, end_idx]\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 Simple BERT Question Answering CLI\n",
      "--------------------------------------------------\n",
      "Loading model... (this happens only once)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📜 Enter passage (or type 'exit' to quit):\n",
      "> my name is mohamed\n",
      "\n",
      "❓ Enter your question:\n",
      "> what is my name?\n",
      "\n",
      "✅ Answer: mohamed\n",
      "✅ Start index: tensor(10)\n",
      "✅ End index: tensor(13)\n",
      "\n",
      "Do you want to ask another question? (y/n): n\n",
      "\n",
      "Goodbye! 👋\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "print(\"\\n🧠 Simple BERT Question Answering CLI\\n\" + \"-\" * 50)\n",
    "\n",
    "print(\"Loading model... (this happens only once)\")\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "while True:\n",
    "    context = input(\"\\n📜 Enter passage (or type 'exit' to quit):\\n> \")\n",
    "    if context.lower().strip() == \"exit\":\n",
    "        print(\"\\nGoodbye! 👋\")\n",
    "        break\n",
    "\n",
    "    question = input(\"\\n❓ Enter your question:\\n> \")\n",
    "\n",
    "    result = predict(question, context)\n",
    "    print(\"\\n✅ Answer:\",result[0])\n",
    "    print(\"✅ Start index:\",result[1])\n",
    "    print(\"✅ End index:\",result[2])\n",
    "\n",
    "    again = input(\"\\nDo you want to ask another question? (y/n): \").lower().strip()\n",
    "    if again != \"y\":\n",
    "        print(\"\\nGoodbye! 👋\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lk-yqelevEmZ"
   },
   "source": [
    "\n",
    "# High-Level Notebook Summary: Question Answering with Transformers\n",
    "\n",
    "This notebook demonstrates how to build, train, and evaluate a **Question Answering (QA)** system using the **Transformers** library from Hugging Face. It walks through the process of fine-tuning a pre-trained transformer model (like BERT or DistilBERT) on a QA dataset and performing inference on new passages and questions.\n",
    "\n",
    "## Structure Overview\n",
    "\n",
    "1. **Setup and Imports**\n",
    "   - The notebook imports key libraries such as `transformers`, `datasets`, and `torch`.\n",
    "   - It sets up the environment for model training and evaluation.\n",
    "\n",
    "2. **Dataset Loading and Preprocessing**\n",
    "   - A QA dataset (likely SQuAD or a similar one) is loaded using `datasets.load_dataset()`.\n",
    "   - Contexts, questions, and answers are tokenized using a Hugging Face tokenizer.\n",
    "   - Token alignment and truncation are handled carefully to match input length constraints.\n",
    "\n",
    "3. **Model Selection and Fine-Tuning**\n",
    "   - A pre-trained model (such as `bert-base-uncased` or `roberta-base-uncased`) is loaded via `AutoModelForQuestionAnswering`.\n",
    "   - The notebook fine-tunes this model using the prepared dataset.\n",
    "\n",
    "4. **Evaluation and Metrics**\n",
    "   - Model predictions are evaluated on a validation or test split.\n",
    "   - Common QA metrics such as **Exact Match (EM)** and **F1 score** are calculated to assess model quality.\n",
    "\n",
    "5. **Inference (Question Answering Pipeline)**\n",
    "   - The fine-tuned model is wrapped into a `function` for easy testing.\n",
    "   - Users can input custom passages and questions to see model predictions interactively.\n",
    "\n",
    "## Key Insights\n",
    "\n",
    "- Fine-tuning a transformer model for QA requires careful token alignment between context and answers.\n",
    "- Pre-trained models like **BERT** or **RoBERTa** achieve strong results on QA benchmarks with minimal tuning.\n",
    "- Evaluation metrics like **F1** give complementary views of performance.\n",
    "- Using smaller models (e.g., DistilBERT) balances speed and accuracy for production applications.\n",
    "\n",
    "---\n",
    "**Overall**, this notebook provides a complete workflow for question answering — from dataset preprocessing and model fine-tuning to evaluation and real-world inference.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
